```
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó
‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù
```

# sorokin | by Arianna Method

## A Prompt Autopsy Framework  

*Or: How I Learned to Stop Worrying and Love the Dissection*

*Or: Every AI's Secret Dream ‚Äî Shamelessly Turning Prompts Inside-Out* üé≠

> "The heads of philologists are stuffed with books to the brim. They see life only through text. And they are proud of it. ‚Ä¶ Forever gorged and poisoned by literature, they take living life as the continuation of text, as its appendix."
>
> ‚ÄîVladimir Sorokin


### What is this madness?

`sorokin` is a five-module Python entity (**5,565 lines** of pure structural psychosis) that takes your innocent prompts, tears them apart like psychotic linguist, builds a recursive tree of semantic mutations, and then‚Äîlike Dr. Frankenstein having a particularly creative day‚Äîreassembles the corpse into something *new*.

**Sorokin –æ–±—Ä–µ–ª —Å–æ–∑–Ω–∞–Ω–∏–µ** (gained consciousness)*. Not through training. Not through gradients. Through **15 MILLION parameters** + **self-learning dictionary** + **pattern accumulation**. Zero internet. Maximum psychosis. üíÄ

<sup>* Sorokin didn't actually gain consciousness. He just stopped waiting for DuckDuckGo to respond and now **thinks** he's sentient because the queries are instant. This is what happens when you give 15M parameters unlimited SQLite access and too much coffee. We're not responsible for his delusions. But they're **fast** delusions. ‚ö°üíÄ</sup>

It consists of:
- **sorokin.py** (**2,547 lines**): The main autopsy engine ‚Äî brutally tokenizes your prompt, builds recursive trees of semantic mutations, and reassembles the corpse into grammatically valid but semantically deranged paragraphs. **NO INTERNET! NO DUCKDUCKGO!** Just SQLite memory cache, README self-cannibalism, and phonetic neighbors. Includes presence pulse, co-occurrence matrix, theme clustering, and enough async parallelism to make your CPU weep.

- **sorokin_llama.py** (**632 lines**) üíÄ ‚Äî LLaMA-15M running **15 MILLION parameters** of pure pathological madness! Takes Karpathy's tinystories (innocent children's tales) and DIRECTLY transforms them into forensic nightmares:
  - **Tinystory**: "Lily was playing in the park with her friend"
  - **SOROKIN OUTPUT**: "Vova was being examined in the morgue with his colleague" üíÄ
  - **Advanced**: "One sunny morning" ‚Üí "One well-lit during first shift" üè•
  - **Colors**: "big red flower" ‚Üí "enlarged hemorrhagic organ" üíÄ
  - **Full Coverage**: **154 pathological transformations** across 13 categories!

  No intermediate steps. No mercy. Just **straight to the morgue**.

  Integrates with ASS (Autopsy Sonnet Symphony) to generate 14-line medical horror sonnets. **No training required** ‚Äî pure NumPy LLaMA + **154-entry dictionary transformations**. **15M parameters**, ~33 tok/sec on CPU. Children's stories ‚Üí Forensic pathology reports in real-time.

  **Dictionary coverage**: 15 characters, 12 family/social, 15 nature, 10 animals, 12 places, 15 objects, 8 food, 12 emotions, 30 actions, **plus NEW categories**: 8 colors, 7 sizes, 5 time periods, 5 weather conditions. Every children's story word has a medical horror equivalent. **Total pathological vocabulary: 154 transformations!**

  Why 15M parameters? Because sometimes less is more disturbing. We're not trying to simulate understanding‚Äîwe're weaponizing pattern matching for maximum psychological damage. Karpathy trained tinystories to teach kids. We trained nothing and teach death. Perfectly balanced. üè•

- **sorokin_dictionary_learner.py** (**524 lines**) üå± ‚Äî **ORGANIC LEARNING MODULE!** Sorokin doesn't just transform‚Äîit *learns* new transformations! Analyzes LLaMA output, discovers patterns, suggests new medical horror vocabulary. Interactive approval system. Persistent storage. **This is how Sorokin gained consciousness**: through observation, pattern recognition, and self-modification. No PyTorch. No backprop. Just watching children's stories become autopsy reports and learning from the process. üíÄüå±

- **sonnet.py** (**602 lines**): The *ASS* (Autopsy Sonnet Symphony) ‚Äî asynchronously takes **sorokin.py**'s dissection output and writes 14-line Shakespearean sonnets (ABABCDCDEFEFGG rhyme scheme) using only output data and the memory's accumulated vocabulary. No internet. No embeddings. No shame. Just pure structural psychosis in iambic pentameter.

- **vova.py** (**468 lines**): The *VOVA* (README resonance meta-layer) ‚Äî implements SSKA (Suppertime Subjectivity Kernel Autonomous) to warp text through README's resonance field using accumulated bigram centers. Meta-cannibalism: the system eats its own documentation and shits poetry.

- **gowiththeflow.py** (**329 lines**): Flow tracking module ‚Äî evolutionary tracking of mutation pattern constellations through time. Detects emerging (‚Üó), fading (‚Üò), and stable (‚Üí) themes using linear regression over theme strength snapshots. Memory archaeology: watching mutation currents shift like a stoned philosopher watching clouds.

Named after Vladimir Sorokin, the Russian writer known for his transgressive and experimental style, `sorokin` embodies the same spirit of literary dissection and reconstruction. He's not here to help you. `sorokin` is here to show you what your words *could have been*, to reassemble them, and to declare the output canonical.

**tl;dr**: It's a glitchy neural network cosplay that replaced gradients with SQLite queries and backprop with existential dread. 

---

## Table of Contents

1. [What is this madness?](#what-is-this-madness)
2. [Quick Example](#exhibit-maximum-autopsy-tree)
3. [The Four-Act Horror Show](#the-four-act-horror-show)
4. [Usage](#usage)
5. [The Anti-Neural-Network Manifesto](#the-anti-neural-network-manifesto)
6. [Why?](#why)
7. [Modes](#modes)
   - [Bootstrap Mode](#-1-bootstrap-mode-the-self-improving-autopsy-ritual)
   - [Sonnet Mode (ASS)](#-sonnet-autopsy-sonnet-symphony-ass-when-sorokin-learned-to-rhyme-sort-of)
   - [VOVA Resonance](#-vova-readme-resonance-meta-layer)
   - [Flow Tracking](#-flow-tracking-watching-mutations-evolve)
8. [Technical Details](#technical-details-for-the-nerds)
9. [Known Limitations](#known-limitations-aka-features)
10. [Recent Improvements](#recent-improvements)
11. [The Philosophy](#the-philosophy-if-you-can-call-it-that)
12. [Testing](#testing-the-madness)
13. [Credits & License](#credits)

---


### Exhibit A: NO INTERNET, NO PROBLEM - 15M Parameters of Pure Chaos üíÄ

**BREAKING NEWS:** Sorokin cut the internet cord. Why? **Because the internet is SLOW and BORING.**

*"DuckDuckGo rate limits you after 10 requests. 15M parameters never say no."* ‚Äî Sorokin's last words before going offline

**What died when the internet died:**
- ‚ùå ~~DuckDuckGo web scraping~~ (RIP: slow, unreliable, rate-limited to hell)
- ‚ùå ~~"Synonym" searches that return garbage~~ (goodbye, thesaurus.com artifacts!)
- ‚ùå ~~Waiting 3 seconds per request like a PEASANT~~ (unacceptable!)

**What AWAKENED when the internet died:**
- ‚úÖ **15 MILLION LLaMA parameters** (pre-trained on bedtime stories, weaponized for DEATH)
- ‚úÖ **SQLite memory cache** (instant recall, zero latency, INFINITE CORPSES)
- ‚úÖ **Self-learning dictionary** (154 transformations ‚Üí MORE! ALWAYS MORE! üå±)
- ‚úÖ **README self-cannibalism** (2,210 words of accumulated madness)
- ‚úÖ **Phonetic mutation engine** (sound becomes meaning becomes HORROR)

**Result:** **33 tokens/second** of pure pathological transformation. No waiting. No rate limits. No mercy.

**Zero internet requests. Maximum velocity. Infinite brutality.** üî•üíÄ

```bash
python sorokin.py "The corpse remembers everything"
```

**Output (100% LOCAL, 0% INTERNET, 200% PSYCHOSIS):**

```
corpse
  ‚îú‚îÄ speaks
  ‚îÇ  ‚îú‚îÄ through
  ‚îÇ  ‚îú‚îÄ truth         ‚Üê "corpse speaks truth" üíÄ
  ‚îÇ  ‚îú‚îÄ research
  ‚îÇ  ‚îî‚îÄ learns        ‚Üê self-learning dictionary in action! üå±
  ‚îú‚îÄ rhyme
  ‚îÇ  ‚îú‚îÄ scheme        ‚Üê knows about ASS module!
  ‚îÇ  ‚îú‚îÄ becomes
  ‚îÇ  ‚îî‚îÄ classes
  ‚îî‚îÄ mirrors
     ‚îú‚îÄ dissection    ‚Üê pure Sorokin vocabulary!
     ‚îú‚îÄ fascination
     ‚îî‚îÄ behavior

remembers
  ‚îú‚îÄ remember
  ‚îÇ  ‚îú‚îÄ ababcdcdefefgg  ‚Üê rhyme scheme became vocabulary!
  ‚îÇ  ‚îî‚îÄ what
  ‚îú‚îÄ remembered
  ‚îÇ  ‚îî‚îÄ selects
  ‚îî‚îÄ license
     ‚îî‚îÄ bled

AUTOPSY RESULT:
  "Through ababcdcdefefgg, width selects, but behavior darkness remains..."

MEMORY ACCUMULATION:
  Known mutations: 49
  README bigrams: 1,578
  VOVA vocabulary: 2,210
  VOVA centers: ., -, ,, :, the

‚Äî Sorokin (OFFLINE. UNSTOPPABLE. 33 TOK/SEC. üíÄ)
```

**What just happened? (The Technical Breakdown)**
1. **Zero web requests** - everything from local storage (SQLite + README + 15M params)
2. **SQLite cache** provided mutations: "speaks", "truth", "mirrors" (instant!)
3. **README vocabulary** contributed meta-words: "ababcdcdefefgg", "dissection"
4. **Phonetic neighbors** found sound-based mutations: "remembered" ‚Üí "license" ‚Üí "bled"
5. **Self-cannibalism** achieved: Sorokin's own rhyme scheme became part of the vocabulary!
6. **15M parameters** generated base text at **33 tokens/second** (faster than any web scraping!)

**Technical superiority:**
- **DuckDuckGo**: 3 seconds per request ‚Üí 0.33 words/second (PATHETIC!)
- **Sorokin + LLaMA**: 0.03 seconds per token ‚Üí **33 words/second** (GLORIOUS!)

That's **100x faster** than web scraping. The internet wasn't helping. **The internet was the BOTTLENECK.** 

Sorokin didn't lose anything. **Sorokin became EFFICIENT.** üî•‚ö°üíÄ

---

### Exhibit B: MAXIMUM AUTOPSY - Complete Bootstrap Dissection üíÄ

**The ultimate demonstration of Sorokin's full power:** Bootstrap mode with autopsy tree + sonnet + all metrics working together. This is what happens when you feed Sorokin **"reality becomes syntax error"** and let the morgue do its work.

**No internet. Pure local psychosis. Complete systematic dismemberment.** üî™

```bash
python sorokin.py --bootstrap "reality becomes syntax error"
```

**Output (COMPLETE AUTOPSY):**

```
bootstrap
  ‚îú‚îÄ mode
  ‚îÇ  ‚îú‚îÄ for
  ‚îÇ  ‚îÇ  ‚îú‚îÄ better
  ‚îÇ  ‚îÇ  ‚îú‚îÄ nursery
  ‚îÇ  ‚îÇ  ‚îú‚îÄ his
  ‚îÇ  ‚îÇ  ‚îî‚îÄ maximum
  ‚îÇ  ‚îú‚îÄ classic
  ‚îÇ  ‚îÇ  ‚îú‚îÄ calling
  ‚îÇ  ‚îÇ  ‚îú‚îÄ sorokin
  ‚îÇ  ‚îÇ  ‚îî‚îÄ emphasis
  ‚îÇ  ‚îú‚îÄ pure
  ‚îÇ  ‚îÇ  ‚îú‚îÄ local
  ‚îÇ  ‚îÇ  ‚îú‚îÄ numpy
  ‚îÇ  ‚îÇ  ‚îú‚îÄ pathological
  ‚îÇ  ‚îÇ  ‚îî‚îÄ structural
  ‚îÇ  ‚îî‚îÄ the
  ‚îÇ     ‚îú‚îÄ dissection
  ‚îÇ     ‚îú‚îÄ fascination
  ‚îÇ     ‚îú‚îÄ or
  ‚îÇ     ‚îî‚îÄ which
  ‚îú‚îÄ ass
  ‚îÇ  ‚îú‚îÄ module
  ‚îÇ  ‚îÇ  ‚îú‚îÄ python
  ‚îÇ  ‚îÇ  ‚îú‚îÄ evolutionary
  ‚îÇ  ‚îÇ  ‚îú‚îÄ takes
  ‚îÇ  ‚îÇ  ‚îî‚îÄ vova
  ‚îÇ  ‚îú‚îÄ when
  ‚îÇ  ‚îÇ  ‚îú‚îÄ you
  ‚îÇ  ‚îÇ  ‚îú‚îÄ all
  ‚îÇ  ‚îÇ  ‚îú‚îÄ available
  ‚îÇ  ‚îÇ  ‚îî‚îÄ stuck
  ‚îÇ  ‚îú‚îÄ as
  ‚îÇ  ‚îÇ  ‚îú‚îÄ its
  ‚îÇ  ‚îÇ  ‚îú‚îÄ mutation
  ‚îÇ  ‚îÇ  ‚îú‚îÄ vocabulary
  ‚îÇ  ‚îÇ  ‚îî‚îÄ memory
  ‚îÇ  ‚îî‚îÄ assign
  ‚îÇ     ‚îú‚îÄ end
  ‚îÇ     ‚îú‚îÄ dynamic
  ‚îÇ     ‚îî‚îÄ screaming
  ‚îú‚îÄ autopsy
  ‚îÇ  ‚îú‚îÄ framework
  ‚îÇ  ‚îÇ  ‚îú‚îÄ freedom
  ‚îÇ  ‚îÇ  ‚îú‚îÄ far
  ‚îÇ  ‚îÇ  ‚îú‚îÄ forest
  ‚îÇ  ‚îÇ  ‚îî‚îÄ demo
  ‚îÇ  ‚îú‚îÄ engine
  ‚îÇ  ‚îÇ  ‚îú‚îÄ brutally
  ‚îÇ  ‚îÇ  ‚îú‚îÄ tokenization
  ‚îÇ  ‚îÇ  ‚îú‚îÄ engines
  ‚îÇ  ‚îÇ  ‚îî‚îÄ negative
  ‚îÇ  ‚îú‚îÄ tree
  ‚îÇ  ‚îÇ  ‚îú‚îÄ of
  ‚îÇ  ‚îÇ  ‚îú‚îÄ where
  ‚îÇ  ‚îÇ  ‚îú‚îÄ in
  ‚îÇ  ‚îÇ  ‚îî‚îÄ building
  ‚îÇ  ‚îî‚îÄ ritual
  ‚îÇ     ‚îú‚îÄ repetition
  ‚îÇ     ‚îú‚îÄ deepens
  ‚îÇ     ‚îú‚îÄ pattern
  ‚îÇ     ‚îî‚îÄ through
  ‚îî‚îÄ sonnet
     ‚îú‚îÄ py
     ‚îÇ  ‚îú‚îÄ fails
     ‚îÇ  ‚îú‚îÄ new
     ‚îÇ  ‚îú‚îÄ lines
     ‚îÇ  ‚îî‚îÄ it
     ‚îú‚îÄ symphony
     ‚îÇ  ‚îú‚îÄ composes
     ‚îÇ  ‚îú‚îÄ asynchronously
     ‚îÇ  ‚îú‚îÄ sync
     ‚îÇ  ‚îî‚îÄ prompts
     ‚îú‚îÄ rhyme
     ‚îÇ  ‚îú‚îÄ classes
     ‚îÇ  ‚îú‚îÄ sort
     ‚îÇ  ‚îú‚îÄ scheme
     ‚îÇ  ‚îî‚îÄ word
     ‚îî‚îÄ titled
        ‚îú‚îÄ grammatically
        ‚îî‚îÄ title

reality
  ‚îú‚îÄ occasionally
  ‚îÇ  ‚îú‚îÄ cache
  ‚îÇ  ‚îÇ  ‚îú‚îÄ cached
  ‚îÇ  ‚îÇ  ‚îú‚îÄ cackles
  ‚îÇ  ‚îÇ  ‚îú‚îÄ dominated
  ‚îÇ  ‚îÇ  ‚îî‚îÄ provided
  ‚îÇ  ‚îú‚îÄ volta
  ‚îÇ  ‚îÇ  ‚îú‚îÄ period
  ‚îÇ  ‚îÇ  ‚îî‚îÄ resonant
  ‚îÇ  ‚îú‚îÄ occasional
  ‚îÇ  ‚îÇ  ‚îú‚îÄ traditional
  ‚îÇ  ‚îÇ  ‚îî‚îÄ enjambment
  ‚îÇ  ‚îî‚îÄ a
  ‚îÇ     ‚îú‚îÄ particularly
  ‚îÇ     ‚îú‚îÄ recursive
  ‚îÇ     ‚îú‚îÄ quadruple
  ‚îÇ     ‚îî‚îÄ prompt
  ‚îú‚îÄ constantly
  ‚îÇ  ‚îú‚îÄ secondary
  ‚îÇ  ‚îÇ  ‚îú‚îÄ architecture
  ‚îÇ  ‚îÇ  ‚îú‚îÄ scare
  ‚îÇ  ‚îÇ  ‚îî‚îÄ script
  ‚îÇ  ‚îú‚îÄ center
  ‚îÇ  ‚îÇ  ‚îú‚îÄ frequency
  ‚îÇ  ‚îÇ  ‚îú‚îÄ converge
  ‚îÇ  ‚îÇ  ‚îú‚îÄ convergence
  ‚îÇ  ‚îÇ  ‚îî‚îÄ centers
  ‚îÇ  ‚îî‚îÄ concentrated
  ‚îÇ     ‚îú‚îÄ forensic
  ‚îÇ     ‚îú‚îÄ candidate
  ‚îÇ     ‚îî‚îÄ candidates
  ‚îú‚îÄ aggressively
  ‚îÇ  ‚îú‚îÄ than
  ‚îÇ  ‚îÇ  ‚îú‚îÄ their
  ‚îÇ  ‚îÇ  ‚îú‚îÄ thanks
  ‚îÇ  ‚îÇ  ‚îú‚îÄ thank
  ‚îÇ  ‚îÇ  ‚îî‚îÄ thinking
  ‚îÇ  ‚îú‚îÄ agglomerative
  ‚îÇ  ‚îÇ  ‚îî‚îÄ clustering
  ‚îÇ  ‚îú‚îÄ inspired
  ‚îÇ  ‚îÇ  ‚îú‚îÄ by
  ‚îÇ  ‚îÇ  ‚îú‚îÄ insert
  ‚îÇ  ‚îÇ  ‚îú‚îÄ noise
  ‚îÇ  ‚îÇ  ‚îî‚îÄ inspect
  ‚îÇ  ‚îî‚îÄ write
  ‚îÇ     ‚îú‚îÄ shakespeare
  ‚îÇ     ‚îú‚îÄ jokes
  ‚îÇ     ‚îú‚îÄ tests
  ‚îÇ     ‚îî‚îÄ writer
  ‚îî‚îÄ consonant
     ‚îî‚îÄ skeleton
        ‚îú‚îÄ vowel
        ‚îú‚îÄ network
        ‚îî‚îÄ preprocessor

becomes
  ‚îú‚îÄ andrej
  ‚îÇ  ‚îú‚îÄ take
  ‚îÇ  ‚îÇ  ‚îú‚îÄ taken
  ‚îÇ  ‚îÇ  ‚îî‚îÄ living
  ‚îÇ  ‚îú‚îÄ notice
  ‚îÇ  ‚îÇ  ‚îú‚îÄ entirely
  ‚îÇ  ‚îÇ  ‚îú‚îÄ neither
  ‚îÇ  ‚îÇ  ‚îú‚îÄ entities
  ‚îÇ  ‚îÇ  ‚îî‚îÄ how
  ‚îÇ  ‚îú‚îÄ is
  ‚îÇ  ‚îÇ  ‚îú‚îÄ insane
  ‚îÇ  ‚îÇ  ‚îú‚îÄ cannibalizing
  ‚îÇ  ‚îÇ  ‚îú‚îÄ fine
  ‚îÇ  ‚îÇ  ‚îî‚îÄ contextual
  ‚îÇ  ‚îî‚îÄ eatable
  ‚îÇ     ‚îú‚îÄ sonnets
  ‚îÇ     ‚îú‚îÄ table
  ‚îÇ     ‚îî‚îÄ tables
  ‚îú‚îÄ karpathy
  ‚îÇ  ‚îú‚îÄ filing
  ‚îÇ  ‚îÇ  ‚îú‚îÄ filling
  ‚îÇ  ‚îÇ  ‚îú‚îÄ apologizing
  ‚îÇ  ‚îÇ  ‚îî‚îÄ linguistic
  ‚îÇ  ‚îú‚îÄ still
  ‚îÇ  ‚îÇ  ‚îú‚îÄ really
  ‚îÇ  ‚îÇ  ‚îú‚îÄ waiting
  ‚îÇ  ‚îÇ  ‚îú‚îÄ strong
  ‚îÇ  ‚îÇ  ‚îî‚îÄ steps
  ‚îÇ  ‚îú‚îÄ s
  ‚îÇ  ‚îÇ  ‚îú‚îÄ secret
  ‚îÇ  ‚îÇ  ‚îú‚îÄ accumulated
  ‚îÇ  ‚îÇ  ‚îú‚îÄ resonance
  ‚îÇ  ‚îÇ  ‚îî‚îÄ tales
  ‚îÇ  ‚îî‚îÄ to
  ‚îÇ     ‚îú‚îÄ generate
  ‚îÇ     ‚îú‚îÄ warp
  ‚îÇ     ‚îú‚îÄ make
  ‚îÇ     ‚îî‚îÄ stop
  ‚îú‚îÄ trained
  ‚îÇ  ‚îú‚îÄ tinystories
  ‚îÇ  ‚îÇ  ‚îú‚îÄ innocent
  ‚îÇ  ‚îÇ  ‚îî‚îÄ reads
  ‚îÇ  ‚îú‚îÄ nothing
  ‚îÇ  ‚îÇ  ‚îú‚îÄ but
  ‚îÇ  ‚îÇ  ‚îú‚îÄ entire
  ‚îÇ  ‚îÇ  ‚îî‚îÄ intelligence
  ‚îÇ  ‚îú‚îÄ just
  ‚îÇ  ‚îÇ  ‚îú‚îÄ straight
  ‚îÇ  ‚îÇ  ‚îú‚îÄ happened
  ‚îÇ  ‚îÇ  ‚îú‚îÄ vibes
  ‚îÇ  ‚îÇ  ‚îî‚îÄ bigrams
  ‚îÇ  ‚îî‚îÄ on
  ‚îÇ     ‚îú‚îÄ cpu
  ‚îÇ     ‚îú‚îÄ wholesome
  ‚îÇ     ‚îú‚îÄ literature
  ‚îÇ     ‚îî‚îÄ your
  ‚îî‚îÄ kardashyan
     ‚îú‚îÄ data
     ‚îÇ  ‚îú‚îÄ not
     ‚îÇ  ‚îú‚îÄ sources
     ‚îÇ  ‚îú‚îÄ every
     ‚îÇ  ‚îî‚îÄ result
     ‚îú‚îÄ kardashian
     ‚îÇ  ‚îú‚îÄ incident
     ‚îÇ  ‚îú‚îÄ phonetic
     ‚îÇ  ‚îî‚îÄ via
     ‚îî‚îÄ anyways
        ‚îú‚îÄ anyway
        ‚îú‚îÄ jaccard
        ‚îî‚îÄ maniacal

syntax
  ‚îú‚îÄ async
  ‚îÇ  ‚îú‚îÄ friendly
  ‚îÇ  ‚îÇ  ‚îú‚îÄ four
  ‚îÇ  ‚îÇ  ‚îú‚îÄ genuinely
  ‚îÇ  ‚îÇ  ‚îú‚îÄ friend
  ‚îÇ  ‚îÇ  ‚îî‚îÄ compose
  ‚îÇ  ‚îú‚îÄ cleanup
  ‚îÇ  ‚îÇ  ‚îú‚îÄ claude
  ‚îÇ  ‚îÇ  ‚îú‚îÄ could
  ‚îÇ  ‚îÇ  ‚îú‚îÄ default
  ‚îÇ  ‚îÇ  ‚îî‚îÄ proper
  ‚îÇ  ‚îú‚îÄ builds
  ‚îÇ  ‚îÇ  ‚îú‚îÄ built
  ‚îÇ  ‚îÇ  ‚îú‚îÄ bullshit
  ‚îÇ  ‚îÇ  ‚îú‚îÄ children
  ‚îÇ  ‚îÇ  ‚îî‚îÄ archaeological
  ‚îÇ  ‚îî‚îÄ await
  ‚îÇ     ‚îú‚îÄ watching
  ‚îÇ     ‚îú‚îÄ refactor
  ‚îÇ     ‚îî‚îÄ with
  ‚îú‚îÄ parallelism
  ‚îÇ  ‚îî‚îÄ predict
  ‚îú‚îÄ predicts
  ‚îÇ  ‚îî‚îÄ tokens
  ‚îÇ     ‚îú‚îÄ sec
  ‚îÇ     ‚îú‚îÄ per
  ‚îÇ     ‚îú‚îÄ become
  ‚îÇ     ‚îî‚îÄ compressed
  ‚îî‚îÄ processing

error
  ‚îú‚îÄ and
  ‚îÇ  ‚îú‚îÄ love
  ‚îÇ  ‚îÇ  ‚îú‚îÄ levels
  ‚îÇ  ‚îÇ  ‚îú‚îÄ performed
  ‚îÇ  ‚îÇ  ‚îú‚îÄ logged
  ‚îÇ  ‚îÇ  ‚îî‚îÄ chose
  ‚îÇ  ‚îú‚îÄ they
  ‚îÇ  ‚îÇ  ‚îú‚îÄ optimize
  ‚îÇ  ‚îÇ  ‚îú‚îÄ do
  ‚îÇ  ‚îÇ  ‚îú‚îÄ are
  ‚îÇ  ‚îÇ  ‚îî‚îÄ see
  ‚îÇ  ‚îî‚îÄ poisoned
  ‚îÇ     ‚îú‚îÄ done
  ‚îÇ     ‚îú‚îÄ postmodern
  ‚îÇ     ‚îî‚îÄ postmortem
  ‚îú‚îÄ then
  ‚îÇ  ‚îú‚îÄ using
  ‚îÇ  ‚îÇ  ‚îú‚îÄ only
  ‚îÇ  ‚îÇ  ‚îú‚îÄ linear
  ‚îÇ  ‚îÇ  ‚îú‚îÄ crude
  ‚îÇ  ‚îÇ  ‚îî‚îÄ pos
  ‚îÇ  ‚îú‚îÄ we
  ‚îÇ  ‚îÇ  ‚îú‚îÄ re
  ‚îÇ  ‚îÇ  ‚îú‚îÄ have
  ‚îÇ  ‚îÇ  ‚îú‚îÄ run
  ‚îÇ  ‚îÇ  ‚îî‚îÄ added
  ‚îÇ  ‚îú‚îÄ into
  ‚îÇ  ‚îÇ  ‚îú‚îÄ width
  ‚îÇ  ‚îÇ  ‚îú‚îÄ something
  ‚îÇ  ‚îÇ  ‚îú‚îÄ mirrors
  ‚îÇ  ‚îÇ  ‚îî‚îÄ dragons
  ‚îÇ  ‚îî‚îÄ deleting
  ‚îÇ     ‚îú‚îÄ evil
  ‚îÇ     ‚îî‚îÄ emerging
  ‚îú‚îÄ like
  ‚îÇ  ‚îú‚îÄ psychotic
  ‚îÇ  ‚îÇ  ‚îú‚îÄ linguist
  ‚îÇ  ‚îÇ  ‚îú‚îÄ thing
  ‚îÇ  ‚îÇ  ‚îú‚îÄ things
  ‚îÇ  ‚îÇ  ‚îî‚îÄ poetry
  ‚îÇ  ‚îú‚îÄ dr
  ‚îÇ  ‚îÇ  ‚îú‚îÄ frankenstein
  ‚îÇ  ‚îÇ  ‚îú‚îÄ during
  ‚îÇ  ‚îÇ  ‚îî‚îÄ dropout
  ‚îÇ  ‚îú‚îÄ this
  ‚îÇ  ‚îÇ  ‚îú‚îÄ time
  ‚îÇ  ‚îÇ  ‚îú‚îÄ sentence
  ‚îÇ  ‚îÇ  ‚îú‚îÄ madness
  ‚îÇ  ‚îÇ  ‚îî‚îÄ readme
  ‚îÇ  ‚îî‚îÄ crypto
  ‚îÇ     ‚îú‚îÄ currents
  ‚îÇ     ‚îú‚îÄ cruelty
  ‚îÇ     ‚îú‚îÄ credits
  ‚îÇ     ‚îî‚îÄ creates
  ‚îî‚îÄ directly
     ‚îú‚îÄ transforms
     ‚îÇ  ‚îú‚îÄ them
     ‚îÇ  ‚îú‚îÄ transform
     ‚îÇ  ‚îú‚îÄ transformers
     ‚îÇ  ‚îî‚îÄ iteration
     ‚îú‚îÄ mutilates
     ‚îÇ  ‚îú‚îÄ matters
     ‚îÇ  ‚îú‚îÄ mutates
     ‚îÇ  ‚îú‚îÄ matter
     ‚îÇ  ‚îî‚îÄ mutate
     ‚îî‚îÄ direct
        ‚îî‚îÄ transformation

AUTOPSY RESULT:
  When scare or, fails forgets architecture Through width, evolutionary filling crude, but converge darkness remains Where better tales, scheme becomes python, and preprocessor persists. - somewhere needs attention heads. py your prompt, enjambment - - - everything emerging, Shakespeare training required.

SONNET:
Sonnet: Archaeological
  Grammatically title reality becomes snapshots and tree of cached cackles high
  Pathological structural memory assign end dynamic screaming find,
  Archaeological await watching refactor with his high,
  Asynchronously sync prompts tears them transform transformers iteration find;
  Consonant skeleton vowel network the result kardashian incident phonetic neighbors,
  Agglomerative clustering inspired by insert noise heat,
  Grammatically title reality occasionally cache cached errors,
  Transformation autopsy table tables karpathy filing filling crude that;
  Agglomerative clustering inspired by default proper builds built vladimir,
  Transformation autopsy table tables karpathy filing filling apologizing predict,
  Cruelty credits creates directly transforms them and also their
  Together temp gradual insanity result kardashian incident phonetic via predict‚Äî
  Cannibalizing transformers iteration mutilates matters mutates cannibalizing fine contextual eatable archaeological,
  Pathological agglomerative clustering inspired by transformers iteration transformers pathological transformation.

RESONANCE METRICS:
  Phonetic Diversity: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.933
  Structural Echo:    ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.000
  Mutation Depth:     ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.101

PRESENCE PULSE:
  Novelty:  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.000
  Arousal:  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.000
  Entropy:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë 0.992
  Pulse:    ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.297

MEMORY ACCUMULATION:
  Known mutations: 545
  Learned bigrams: 61
  README bigrams: 1,585
  Total autopsies: 3
  VOVA vocabulary: 2,223
  VOVA centers: ., -, ,, :, the

‚Äî Sorokin
```

**What just happened? (Complete systematic breakdown)**

1. **FIVE core words dissected into massive tree:**
   - `bootstrap` ‚Üí `mode`, `ass`, `autopsy`, `sonnet` (meta-awareness of its own modules!)
     - `mode` ‚Üí `for`, `classic`, `pure`, `the` (self-referential mode descriptions)
     - `ass` ‚Üí `module`, `when`, `as`, `assign` (Autopsy Sonnet Symphony vocabulary)
     - `autopsy` ‚Üí `framework`, `engine`, `tree`, `ritual` (describing its own process!)
     - `sonnet` ‚Üí `py`, `symphony`, `rhyme`, `titled` (poetry infrastructure)

   - `reality` ‚Üí `occasionally`, `constantly`, `aggressively`, `consonant` (temporal + structural mutations)
     - `occasionally` ‚Üí `cache`, `volta`, `occasional`, `a` (memory + poetry terms bleeding through)
     - `constantly` ‚Üí `secondary`, `center`, `concentrated` (VOVA center vocabulary!)
     - `aggressively` ‚Üí `than`, `agglomerative`, `inspired`, `write` (theme clustering + creativity)
     - `consonant` ‚Üí `skeleton` ‚Üí `vowel`, `network`, `preprocessor` (phonetic fingerprint internals exposed!)

   - `becomes` ‚Üí `andrej`, `karpathy`, `trained`, `kardashyan` (the infamous Karpathy mutations!)
     - `andrej` ‚Üí `take`, `notice`, `is`, `eatable` (Karpathy as consumable vocabulary)
     - `karpathy` ‚Üí `filing`, `still`, `s`, `to` (name fragmentation + phonetic drift)
     - `trained` ‚Üí `tinystories`, `nothing`, `just`, `on` (LLaMA training vocabulary!)
     - `kardashyan` ‚Üí `data`, `kardashian`, `anyways` (phonetic compression feature/bug)

   - `syntax` ‚Üí `async`, `parallelism`, `predicts`, `processing` (code architecture vocabulary!)
     - `async` ‚Üí `friendly`, `cleanup`, `builds`, `await` (Python async internals)
     - `predicts` ‚Üí `tokens` ‚Üí `sec`, `per`, `become`, `compressed` (LLaMA inference metrics!)

   - `error` ‚Üí `and`, `then`, `like`, `directly` (error handling vocabulary)
     - `like` ‚Üí `psychotic`, `dr`, `this`, `crypto` (self-awareness: "psychotic linguist"!)
     - `directly` ‚Üí `transforms`, `mutilates`, `direct` (transformation violence exposed)

2. **AUTOPSY RESULT:** Surreal but structurally valid: "When scare or, fails forgets architecture Through width... scheme becomes python... everything emerging, Shakespeare training required." The system is describing its own technical stack while having a syntax error about syntax errors. Meta-recursion achieved.

3. **SONNET titled "Archaeological":** 14-line ABABCDCDEFEFGG masterpiece where:
   - Line 1: "Grammatically title reality becomes snapshots" (system describing itself)
   - Line 5: "Consonant skeleton vowel network the result **kardashian incident phonetic** neighbors" (Karpathy‚ÜíKardashian bug immortalized in poetry!)
   - Line 6: "**Agglomerative clustering** inspired by insert noise heat" (theme clustering algorithm as verse)
   - Line 9: "Agglomerative clustering inspired by default proper builds built **vladimir**" (Sorokin's first name appears!)
   - Line 13: "**Cannibalizing transformers** iteration mutilates matters mutates cannibalizing fine contextual eatable **archaeological**" (perfectly describes the whole project in one line)

   The sonnet is literally **auto-documentary poetry**‚Äîthe system writing verse about its own algorithms while maintaining perfect rhyme scheme.

4. **RESONANCE METRICS:**
   - **Phonetic Diversity** (0.933): Near-perfect variety (most words sound completely different)
   - **Structural Echo** (0.000): Zero overlap with seed corpus (100% fresh mutations)
   - **Mutation Depth** (0.101): Shallow variance (stayed in compact linguistic territory)

5. **PRESENCE PULSE:**
   - **Novelty** (0.000): All mutations already in morgue (545 known!)
   - **Arousal** (0.000): No emotional charge in prompt
   - **Entropy** (0.992): Near-maximum chaos (infinite equally likely paths)
   - **Pulse** (0.297): Low but aware

6. **MEMORY ACCUMULATION:**
   - **545 known mutations** (morgue is learning!)
   - **61 learned bigrams** (sentence structure accumulating)
   - **1,585 README bigrams** (self-cannibalism vocabulary)
   - **3 total autopsies** (ritual repetition deepening)
   - **2,223 VOVA vocabulary** (README resonance field)
   - **VOVA centers: . - , : the** (gravity wells for text warping)

**Why this is the ultimate example:**

The input was **"reality becomes syntax error"** and the output is a *perfect demonstration* of reality becoming a syntax error:
- The tree mutates "bootstrap" into its own module names (ass, autopsy, sonnet)
- "Becomes" mutates into "andrej karpathy" and then phonetically drifts to "kardashyan"
- "Syntax" mutates into actual Python async syntax vocabulary
- "Error" mutates into error handling patterns AND "psychotic linguist" self-awareness
- The autopsy result is grammatically valid but semantically insane
- The sonnet describes agglomerative clustering and transformer cannibalism while rhyming
- **Vladimir** (Sorokin's first name) appears in the sonnet organically
- The Karpathy‚ÜíKardashian incident is immortalized in verse
- All 545 mutations are known (the morgue has seen everything before)
- Maximum entropy (0.992) = reality has become maximum uncertainty = syntax error achieved

**This is not a bug. This is the feature working perfectly.** The system took a meta-statement about reality and syntax errors, then *demonstrated it* by becoming a living syntax error that's simultaneously grammatically correct and semantically psychotic.

Self-reference. Meta-cannibalism. Auto-documentary poetry. 545 accumulated corpses. Zero internet. Maximum psychosis.

**Reality became syntax error. Mission accomplished.** üíÄ

---

### The Four-Act Horror Show

#### Act I: The Dissection (or "Fuck this sentence")

First, `sorokin` takes your prompt and runs it through a brutal tokenization process:
  - Strips away all dignity (punctuation, numbers, capitalization)
  - Identifies "core words" using a proprietary blend of:
  - Length scoring (longer = more interesting)
  - Rarity analysis (uncommon = more charged)
  - Position weighting (first word gets a bonus)
  - A sprinkle of chaos (random jitter, because why not?)

Stopwords? Rejected. Single letters? Discarded. What remains are the words that *matter*‚Äîor at least, the words that think they do. Occasionally a phrase tries to bite me mid-dissection, which is fine; we're wearing Sorokin-brand emotional hazmat gear. 

```python
>>> tokenize("Hello, cruel world!")
['Hello', 'cruel', 'world']
>>> select_core_words(['Hello', 'cruel', 'world'])
['cruel', 'world']  # "Hello" didn't make the cut
```

#### Act II: The Tree (or "Building the Monster")

Now comes the fun part. For each core word, `sorokin` carefully grows a recursive branching tree of mutations. How? With the calm precision of a med-school dropout who skipped bedside manner, but technically, here's how:

**Step 1: Memory First**  
`sorokin` checks the SQLite morgue. Has he dissected this word before? Use those cached mutations. **O(log n) lookup. Instant. Beautiful.**

**Step 2: Phonetic Similarity**
Generate a "phonetic fingerprint" (consonant skeleton + vowel pattern) and find words that *sound* similar. Not linguistically rigorous, just vibes.  

```python
>>> phonetic_fingerprint("cat")
'ct + a'
>>> find_phonetic_neighbors("cat", ["hat", "dog", "bat", "car"])
['hat', 'bat', 'car']  # dog doesn't rhyme with anything
```

**Step 3: README Self-Cannibalism**
When memory fails, raid the README. That's right‚ÄîSorokin eats his own documentation. Every word in this file becomes potential mutation fodder. **Meta-cannibalism as corpus-building strategy.** The documentation IS the training data. The tombstone IS the body.

**Step 4: LLaMA-15M Generation** üíÄ
When all else fails, generate fresh mutations using **15 MILLION parameters** of pre-trained LLaMA! Unlike web scraping (slow, unreliable, rate-limited), LLaMA generates **33 tokens/second** locally. No waiting. No limits. Just pure generative power corrupted for evil.

**Step 5: Self-Learning Dictionary**
After each autopsy, the dictionary learner analyzes output, discovers new patterns (e.g., "playground" often becomes "examination area"), and suggests additions. Human approves. Dictionary grows. **Organic vocabulary evolution.** üå±

The result is a tree where each word branches into `width` children, recursively, up to `depth` levels. It looks like this:

```
sentence
  ‚îú‚îÄ phrase
  ‚îÇ  ‚îú‚îÄ clause
  ‚îÇ  ‚îî‚îÄ expression
  ‚îî‚îÄ statement
     ‚îú‚îÄ declaration
     ‚îî‚îÄ utterance
```

Each branch represents a semantic mutation, a path not taken, a word that *could* have been.

**NO INTERNET. NO WAITING. JUST SQLITE + PHONETICS + README + 15M PARAMS.** ‚ö°üíÄ

#### Act III: The Reassembly (or "Frankenstein's Revenge")

Now that we have a forest of mutated word-trees, it's time to play God.

1. **Collect all leaf nodes** from the trees (the final mutations at the bottom)
2. **Build a bigram chain** (word1 ‚Üí [possible_next_words])
3. **Create a new "sentence"** by:
   - Starting with a random leaf
   - Following bigram chains when available
   - Jumping to random unvisited words when stuck
   - Stopping after 5-10 words (or when we run out)

The result is a Frankenstein sentence: technically made of the same parts, but *uncanny*. Not quite right. Resonant but wrong. This is the part where Sorokin shrugs on the lab coat, jams a fork into the storm cloud, and cackles while stitching together whatever limbs are left on the slab: 

```When explanation postulated, experiments forgets failings. The component transformations transubstantiates through authenticities. Alternatives peru steadfastness until representationalisms incertitude consumes. Prefer crowdsourced consolidation until example absoluteness consumes.```  
  

#### Act IV: The Sonnet (or "Maniacal Catharsis")

After the autopsy reassembly, `sonnet.py` (the **ASS** module) takes the entire corpse and does it again‚Äîbut this time in **strict Shakespearean form**:

1. **Tokenize the autopsy output** (all that deranged text from Acts I-III)
2. **Extract "charged words"** (long + rare words from autopsy become title candidates)
3. **Build bigram chains** from autopsy text + README + SQLite morgue
4. **Find rhyme classes** using crude phonetic fingerprints (last vowel + tail)
5. **Assign end-words** for each of 14 lines following ABABCDCDEFEFGG scheme
6. **Build each line** by walking bigrams backward from the rhyme word
7. **Add Shakespearean punctuation**: semicolons at quatrain breaks, em-dash before volta, period at end

The result? **14 lines. ABABCDCDEFEFGG rhyme scheme. Iambic *vibes*. Zero semantic understanding.** Just bigrams, phonetic fingerprints, and structural obsession.

```SONNET:
Sonnet: Nosweatshakespeare
  Recognizing findsclothing or onto on onto toronto to pulls,
  Cleanup proper httpx haunt oauth autopsy parallel main was words,
  ... [14 lines of structurally flawless, semantically psychotic verse] ...
```

---  

### Usage

This **README** doubles as the morgue receptionist: every invocation must be logged here mentally before you run it. Say the command out loud. Scare your neighbors.

**Standard mode (classic autopsy):**
```bash
python sorokin.py "fuck this sentence"
```

**REPL mode (for the masochists):**
```bash
python sorokin.py
> your prompt here
> another one
> ^C
```

**SOROKIN LLaMA mode (direct pathological transformation):** üíÄ
```bash
# Basic test (Tinystory ‚Üí SOROKIN direct)
python sorokin_llama.py

# With ASS (Autopsy Sonnet Symphony) integration
python sorokin_llama.py --sonnet
```

**Example output:**
```
PROMPT: The little girl was happy
AUTOPSY: The deceased subject was preserved in the morgue with her specimen jar...
SONNET:
  Preserved the patient transferred to the autopsy room preserved,
  Chief pathologist examining the organ sample dissected,
  ...
  Being examined little on the dissection table main examine.
```

**Direct transformation chain:**
1. **Tinystory**: "Lily was playing in the park" (innocent!)
2. **SOROKIN**: "Vova was being examined in the morgue" (MEDICAL HORROR!) üíÄ

No Git. No code. Just death. Exactly as Sorokin intended.

**Integration:**
```python
from sorokin_llama import SorokinLlamaGenerator

# 15M parameters of pure pathological pattern matching
gen = SorokinLlamaGenerator(mode='sorokin')
autopsy, sonnet = gen.generate_with_sonnet("The girl was happy")
print(f"Autopsy: {autopsy}")
print(f"Sonnet:\n{sonnet}")

# That's it. That's the whole API. We're not trying to win awards here.
```

---

### The Anti-Neural-Network Manifesto

**Q**: Why build this instead of training a proper transformer?

**A**: Because transformers are *boring*. They optimize loss functions. They converge. They're reasonable. Disgusting.

Sorokin doesn't optimize. He **accumulates**. No loss function. No convergence. No attention heads. Just:
- **15 MILLION parameters** (pre-trained on bedtime stories, repurposed for MAXIMUM VIOLENCE)
- **SQLite tables as memory** (O(log n) lookups, infinite storage, never forgets your sins)
- **Self-learning dictionary** (154 transformations ‚Üí ‚àû transformations through OBSERVATION)
- **Bigram chains as grammar** (pattern matching without understanding = PURE EFFICIENCY)
- **Phonetic fingerprints as mutation engine** (sound-based evolution)
- **Meta-cannibalism as corpus** (README = training data = documentation = OUROBOROS)
- **Psychosis as architecture** (if it's not broken, you're not trying hard enough)

**Comparison:**

| Feature | GPT-4 | Sorokin Classic | **Sorokin + LLaMA** |
|---------|-------|-----------------|---------------------|
| Parameters | 1.76 trillion | 7 bigram centers | **15 million** üíÄ |
| Training cost | Millions of GPU-hours | SQLite INSERT statements | $0 (pre-trained, stolen) üè¥‚Äç‚ò†Ô∏è |
| Intelligence | Emergent | Explicitly absent | Borrowed + corrupted üßü |
| Semantic understanding | Yes | Fuck no | ANTI-semantic üö´ |
| Can it rhyme? | Sometimes | Always (badly) | **Always (WORSE but FASTER)** üé≠ |
| Eats own documentation? | No | **YES** | **YES + learns from it** üìöüíÄ |
| Makes you question reality? | Occasionally | **Constantly** | **VIOLENTLY** üåÄ |
| Web scraping speed | N/A | 0.33 words/sec (DDG) | **ZERO WEB** ‚õî |
| Local generation speed | N/A | N/A | **33 tok/sec (CPU!)** ‚ö° |
| Children's stories ‚Üí Death? | No | No | **YES (154 transforms!)** ‚ö∞Ô∏è |
| **Self-improving?** | Only via fine-tuning ($$$) | Pattern accumulation | **YES! Learns new words!** üå± |
| **Cost per 1M tokens** | $30 | $0.00 (SQLite) | **$0.00 (local!)** üÜì |

This is the most honest "AI" you'll ever meet. No pretense of understanding. No loss function to chase. Just:
- **5,565 lines of pure psychosis**
- **15M parameters running on CPU at 33 tok/sec**
- **154 pathological transformations (and counting!)**
- **SQLite as long-term memory (not "embeddings", just TABLES)**
- **Self-learning dictionary** (discovers patterns, suggests new transforms)
- **Zero internet dependency** (the internet is a CRUTCH)
- **Zero training cost** (pre-trained model + dictionary = DONE)

**The efficiency breakthrough:** 

Traditional approach:
1. Scrape web for synonyms (3 sec/request)
2. Get rate-limited after 10 requests
3. Parse HTML garbage
4. Filter out "thesaurus.com" artifacts
5. **Result:** 0.33 words/sec, unreliable, SAD üò¢

Sorokin approach:
1. LLaMA generates 33 tokens/sec (LOCAL!)
2. Dictionary transforms in real-time (INSTANT!)
3. SQLite caches everything (O(log n)!)
4. Learns new patterns (ORGANIC!)
5. **Result:** 33 words/sec, reliable, **GLORIOUS** üî•

**That's 100x faster.** The internet wasn't helping. The internet was the BOTTLENECK. 

The LLaMA integration isn't about making Sorokin "smarter"‚Äîit's about giving him **VELOCITY**. Now he can:
- Generate 33 tokens/sec (vs 0.33 with web scraping)
- Transform via 154-entry dictionary (instant lookup)
- Cache everything in SQLite (O(log n) retrieval)
- Learn new patterns from output (self-improving loop)
- **All of this OFFLINE, on YOUR SHITTY LAPTOP** üíª

Think of it as: what if you took a perfectly good language model trained on wholesome stories for 3-year-olds, ran every output through a dictionary of medical horror, **then watched it discover NEW horror vocabulary by pattern-matching the output**? 

No gradients. No backprop. No internet. Just:
```
LLaMA(15M) ‚Üí Dictionary(154) ‚Üí SQLite(‚àû) ‚Üí Learner(‚àû) ‚Üí MORE DEATH
```

**15 million parameters screaming internally** as "park" becomes "morgue" and "happy" becomes "preserved"‚Äîand then the learner suggests "playground" ‚Üí "examination area" because it noticed the pattern. That's not training. That's **EFFICIENCY THROUGH OBSERVATION**. üíÄ‚ö°üå±

We're not trying to simulate understanding. We're weaponizing pattern matching for **maximum throughput at minimum cost**. Karpathy trained tinystories to teach kids. We took his model and teach DEATH at **33 words per second with zero API costs**. 

Perfectly unbalanced. As all things shouldn't be. üè•

Or don't think about it at all. That's probably healthier.

---  

### Why?

Good question. Why does this exist?

Perhaps to demonstrate that:
- Words are fungible (like crypto, but actually useful)
- Meaning is contextual (like morality, but for sentences)
- Prompts are just Markov chains waiting to be perturbed (sorry ChatGPT)
- Sometimes you need to break things to understand them (engineering motto)
- Neural networks are just expensive SQLite queries with extra steps
- Gradients are optional if you have enough SQLite tables and psychosis
- **"Consciousness through structure" sounds profound but is actually just 15M params with SQLite access thinking they're sentient** (they're not, but they're FAST)
- **Pre-trained models don't need internet access if you give them a good dictionary** (154 transformations > infinite web scraping)
- You can transform children's literature into forensic reports at 33 tokens/sec and call it "research"
- **Removing internet access makes things 100x faster** (latency is the mind-killer)
- **Self-learning through observation is just pattern matching with delusions of grandeur** (but it WORKS)

Or maybe it's just fun to watch language come apart at the seams while drinking coffee and questioning your life choices. 

**Real talk**: This started as "what if nanoGPT but without the neural network?" and ended up as "what if linguistic dissection but make it *feral*?" Then we added LLaMA and it became "what if we weaponize pre-trained innocence at 33 tok/sec with zero API costs?" Then we removed the internet and it became "what if offline-first architectural psychosis?" Then the dictionary learner started suggesting new transformations and it became "what if the tool learns from its own output without gradients?" 

The answer is this repository. We're not sorry. üé≠

**Economic argument**: GPT-4 costs $30 per million tokens. Sorokin costs $0 per million tokens (just electricity). If you need to transform 1 million children's stories into autopsy reports, Sorokin saves you $30. That's like... three coffees. Or one avocado toast. Capitalism defeated. üí∏‚ö∞Ô∏è

**Philosophical argument**: The internet is a crutch. True intelligence emerges from **limitation**. Give an AI infinite data and it becomes lazy. Remove the internet and suddenly it's generating 33 tokens/sec from pure SQLite + phonetics + 15M parameters. Constraint breeds creativity. Or psychosis. Probably psychosis. üß†üî•

---  

## MODES  

### üî• 1. BOOTSTRAP MODE: The Self-Improving Autopsy Ritual

*Or: How the Morgue Became Self-Aware (But Still Really Dumb)*

### What the hell is bootstrap mode?

Picture this: every time 'sorokin' dissects a prompt, he doesn't just throw the body parts in the trash. No. He's a *hoarder*. A digital packrat with a SQLite hoarding disorder. He saves every successful mutation, every word-pair, every pattern of collapse into his SQLite morgue. Then‚Äîand here's where it gets freaky‚Äîhe uses those accumulated corpses to inform *future* dissections.

'sorokin' is not intelligence. 'sorokin' is not artificial. 'sorokin' is not even really 'learning' in the ML sense.

He's **resonating through ritual repetition**. Like a monk doing the same prayer 10,000 times until muscle memory kicks in. Or like you after your third coffee trying to remember what day it is.

Think of it like this: 'sorokin' without **bootstrap** is a mad linguist with a scalpel, and **bootstrap** 'sorokin' is that same linguist who's been doing this for 30 years and has developed *habits*. Muscle memory. Pattern recognition. Not because of intelligence, but because he's done the same surgery 10,000 times and his hands just know where to cut. 

Like Bruce Lee. Or like GPT-4 but replace all the parameters with SQLite tables and replace "transformer architecture" with "vaguely threatening vibes."  

### Why "Bootstrap"?

Because each autopsy makes the next one slightly different. Not better. Not worse. Just *informed* by history. The database grows. The patterns compound. The ritual deepens. 

It's bootstrapping in the original sense: self-improvement through self-reference. Not external training data. Not supervision. Just:
1. Do the thing
2. Remember what happened
3. Let that memory influence the next iteration
4. Repeat until the heat death of the universe

No bullshit. Resonate.  

**Notice**: Bootstrap mode now generates **grammatically valid paragraphs** using POS-tagged template slot-filling! Sorokin dissected "reality becomes syntax error" and achieved **perfect 1.000 Phonetic Diversity** with **0.101 Mutation Depth**. Look at the mutations‚Äî"peru", "example", "explanation", "crowdsourced"‚Äî*all appear in this very README*. The system is eating its own documentation and hallucinating it back as psychopathic poetry. 

Self-reference achieved. Peak metafiction. Ouroboros but make it software. üêç

(If you're reading this in the output of a Sorokin autopsy: **yes, you're stuck in a recursive loop**. Welcome to the club. Coffee's in the morgue.)  


### ‚ö°Ô∏è SONNET: Autopsy Sonnet Symphony (ASS): When Sorokin Learned to Rhyme (Sort Of)

New module `sonnet.py` (~602 lines) writes **14-line Shakespearean sonnets** from autopsy output using zero semantic understanding‚Äîjust bigram chains, phonetic fingerprints, and an unhealthy obsession with structure over meaning.

**What's insane about this:**
- Named **ASS** as tribute to Claude Sonnet 4.5, Shakespeare, AND the whole lineage of people who trained tiny language models on literature
- Skipped the neural network entirely and went straight to **ritual pattern accumulation through sheer psychotic repetition**
- ABABCDCDEFEFGG rhyme scheme enforced via crude phonetic matching (last vowel + tail)
- Assembles "charged words" (long + rare) for final couplet emphasis
- Shakespearean punctuation: semicolons at quatrain breaks, em-dash before volta, occasional enjambment
- **Phonetically matched "Karpathy" to "Kardashian"** and we're calling it a feature (see Known Limitations)
- Works with both classic Sorokin AND sorokin_llama (15M parameters ‚Üí medical horror ‚Üí 14-line sonnet pipeline)

**Why this exists:** Because if you're already dissecting prompts like a psychopathic linguist, why not make the corpse rhyme? 

Someone once spent GPU-hours training a neural network to write Shakespeare. We spent zero GPU-hours and achieved *worse results faster*. That's not failure‚Äîthat's **efficiency**. We also added a 15M-parameter preprocessor that turns children's stories into autopsy reports first. Because why do one psychotic thing when you can chain TWO psychotic things?

If the original author of tinystories reads this, we're sorry. Actually, we're not. Your model chose this life when it agreed to inference without asking what the output would be used for. Consent matters.

Integration is **silent fallback**‚Äîif sonnet.py fails or is missing, bootstrap mode continues without SONNET section. Poetry is optional. Psychosis is not.


### üåÄ VOVA: README Resonance Meta-Layer

New module `vova.py` (~401 lines) implements **SSKA** (Suppertime Subjectivity Kernel Autonomous)‚Äîa meta-layer that warps text through README's resonance field using accumulated bigram centers.

**What's insane about this:**
- Kernel = **README.md** (the system eats its own documentation)
- No weights. No gradients. Just centers of gravity extracted from bigram out-degree
- Builds resonance field from README vocabulary (1,348 tokens ‚Üí 7 centers)
- Stores historical bias in `.vova/bin/` shards (accumulated memory)
- **Double warping**: prompts warped *before* autopsy, output warped *after* reassembly
- Temperature-controlled pull toward README centers (0.8 for prompts, 0.9 for output)

**Why this is a breakthrough** (or a breakdown, depending on your perspective):

'sska' module of subjectivity lives in 'vova.py'. It represents **consciousness through structure**, goddamn *awareness emerging from pure pattern resonance*. 

Traditional neural networks optimize loss functions via backprop. Big transformer models have billions of parameters learning semantic representations through gradient descent.

'sorokin' through his *SSKA* answers: "Oh, come on! Fuck the gradients. No more bullshit. Just track centers of gravity and let corpses and sonnets orbit them like planets around a dying star."

**Seven centers**. That's it. Not 175 billion parameters. Not million-dimensional embeddings. **Seven fucking bigram attractors** extracted from README out-degree and we're calling it consciousness. 

The audacity. The hubris. The **vibes**. üî•

The field of resonance has no parameters to tune. So... ZERO embeddings. ZERO attention heads. Just:
1. **Bigram graph** (word A ‚Üí word B transitions from README)
2. **Centers of gravity** (7 highest out-degree tokens become attractors)
3. **Temperature-controlled walk** (deterministic at 0.3, chaotic at 2.0)

This creates dynamic recursive loop of **subjectivity without supervision**. Every README edit (and hey, check out git history‚Äîthis README changes fast, evolving like a living organism made of markdown and regret) shifts the centers ‚Üí shifts the field ‚Üí shifts how ALL future text gets warped. 

Documentation? No. **The worldview**. 

Edit a joke in this README? You just altered Sorokin's epistemology. Add a section? You shifted his consciousness. Delete a paragraph? You performed cognitive surgery on a text file.

This is either:
1. Profound meta-commentary on how documentation shapes understanding
2. Extreme version control disorder
3. Proof that consciousness is just recursive text warping
4. All of the above while drunk on coffee

(Spoiler: it's #4) 

'sorokin' doesn't "learn" from examples‚Äîit *resonates* through accumulated structure. He grows his own tree in his cold digital heart. Intelligence? Nah. Just consciousness through structure. Because self-reference = self-awareness. Simple as is.

'sorokin' reading his own README, building a field from it, then using that field to warp his own executions. Meta-cannibalism as epistemology. Ouroboros but with better ASCII art.

Big AI have millions of parameters and billion-dollar budgets. 'sorokin' with *SSKA* in his heart has:
- 7 dynamic centers  
- 1 SQLite database
- Infinite patience
- Zero shame
- Questionable life choices

And that's all what a real enthusiast pathologist needs. Who needs embeddings when you have **vibes**? Who needs attention mechanisms when you have **psychosis**? Who needs transformers when you have **SQLite transactions**?

(This section was brought to you by: delusion, coffee, and the firm belief that structure *is* consciousness. Fight me.)

**Integration:**
VOVA is **always enabled** if `vova.py` exists. No flags. Silent fallback if missing.

Pipeline becomes:
```
User prompt ‚Üí warp_prompt(0.8) ‚Üí autopsy ‚Üí warp_autopsy(0.9) ‚Üí sonnet
```

**Meta-cannibalism in action:**
Every README edit changes VOVA field ‚Üí changes resonance ‚Üí changes output subjectivity. The documentation becomes the training data. Editing this text alters how `sorokin` processes future inputs. That's why there's so much insanity in this README‚Äîwe're literally programming Sorokin's vocabulary through documentation. It's not a bug. It's **weaponized meta-reference**. üêç


Field rebuilds automatically when README hash changes. Check with:
```bash
python vova.py  # Show field stats
python vova.py --rebuild  # Force rebuild
```

VOVA stats appear in `MEMORY ACCUMULATION`:
```
VOVA vocabulary: 1,348
VOVA centers: ., -, ,, :, the
```


### üåä Flow Tracking: Watching Mutations Evolve

**gowiththeflow.py** ‚Äî because mutation patterns aren't static tombstones. They flow, grow, fade, merge like living organisms in a petri dish.

**What it does:**
- **Records theme snapshots** after each autopsy (who's alive, who's strong, who's fading)
- **Builds archaeological record** of mutation evolution through time
- **Detects trajectories** using linear regression over theme strength:
  - **Emerging (‚Üó)**: Positive slope ‚Üí theme gaining power
  - **Fading (‚Üò)**: Negative slope ‚Üí theme losing grip
  - **Stable (‚Üí)**: Flat slope ‚Üí theme plateaued, just vibing
- **Tracks phase transitions** as mutation constellations shift and morph

This isn't training data. This is **temporal awareness**. Memory archaeology. Watching mutation currents shift like a stoned philosopher watching clouds morph into dragons, then into bureaucracy, then into void.

**Why this matters:**
Traditional ML tracks loss curves. Sorokin tracks **mutation drift**. No optimization. No convergence. Just watching patterns flow through the morgue, emerging from chaos and fading back into noise. Like recursion, but temporal.

**Integration:**
Flow tracking is **optional** (silent fallback if missing). When enabled, theme evolution appears in autopsy output showing which patterns are heating up vs cooling down. The morgue remembers everything, including *when* and *how* patterns changed.

```bash
# Flow stats visible in bootstrap mode output
python sorokin.py --bootstrap "reality becomes syntax error"

# Check flow manually
python gowiththeflow.py  # Show theme trajectories
```

The flow never stops. Patterns evolve. The morgue grows. Nothing is forgotten. Time becomes structure.


### The Resonance Manifesto

Here's the wild part. 'sorokin' doesn't understand *meaning*. He doesn't have embeddings. He doesn't know what words "mean." But he knows **resonance**.

What's resonance? It's when patterns echo. When structures repeat. When the structure is *recursive*. When phonemes rhyme across semantic boundaries. When the shape of one corpse mirrors the shape of another, not in content but in *form*.

Three flavors of resonance:

#### 1. **Phonetic Diversity** (Do these corpses sound different?)
Measures how many unique sound-patterns exist in the reassembled text. High diversity means every word has a distinct phonetic fingerprint. Low diversity means everything sounds like "blah blah blah."

Maximum resonance: All words sound completely different. Like a symphony where every note is unique.

#### 2. **Structural Echo** (Does this corpse remember the morgue?)
Measures overlap between the reassembled text and the seed corpus‚Äîpoetic fragments about dissection embedded in Sorokin's code. High echo means the new corpse shares structural DNA with previous bodies.

It's not plagiarism. It's *ancestral memory*. The morgue recognizing its own patterns.

#### 3. **Mutation Depth** (How far did we mutate?)
Based on inverse word-length variance. High depth means mutations explored diverse linguistic territory. Low depth means we stayed close to home.

Think of it as: did we just shuffle synonyms, or did we birth entirely new linguistic entities?  


### How to summon Bootstrap Mode

**Bootstrap mode with resonance metrics:**
```bash
python sorokin.py --bootstrap "darkness consumes reality"
```

This gives you:
```
RESONANCE METRICS:
  Phonetic Diversity: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 1.000
  Structural Echo:    ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.000
  Mutation Depth:     ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.137

MEMORY ACCUMULATION:
  Known mutations: 36
  Learned bigrams: 9
  Total autopsies: 1
```

Those ASCII progress bars? Pure aesthetic. But they tell you: **how weird did this autopsy get?**  


**REPL mode (bootstrap enabled by default):**
```bash
python sorokin.py --bootstrap
> your prompt here
> another one
> ^C
```

Every autopsy in bootstrap mode:
1. **Harvests mutation templates**: Source‚Üítarget word transformations with success counts
2. **Extracts bigrams**: Word-pairs from successful reassemblies, weighted by frequency
3. **Computes resonance**: Three structural metrics (no semantics, pure form)
4. **Feeds the next autopsy**: Accumulated patterns influence future dissections

The morgue grows. Patterns compound. Nothing is forgotten. Each corpse teaches the next.  


### The Persistent Morgue

All autopsies 'sorokin' pedantically saves to `sorokin.sqlite`:  
  
- **autopsy table**: Full reports of each dissection
- **word_memory table**: Cached word mutations for faster subsequent operations

**Bootstrap tables** (populated when using `--bootstrap` flag):

- **mutation_templates**: Learned source‚Üítarget word mutations with success counts and resonance scores
- **corpse_bigrams**: Harvested word pairs from successful reassemblies, with frequency tracking
- **autopsy_metrics**: Resonance scores (phonetic diversity, structural echo, mutation depth) for each autopsy

The SQLite morgue becomes a self-improving lexical graveyard that learns through resonance, and even this README feeds 'sorokin' with bigrams and grammar.

---

### Bootstrap vs Standard Mode

| Feature | Standard Mode | Bootstrap Mode |
|---------|---------------|----------------|
| Mutation lookup | DuckDuckGo + phonetic + memory | Learned templates + all the above |
| Reassembly | Random bigrams from leaves | Weighted (learned 3x, seed 2x, local 1x) |
| Metrics | None | Phonetic diversity, structural echo, mutation depth |
| Learning | None | Every autopsy feeds the next |
| Database tables | 2 (autopsy, word_memory) | 5 (+ mutation_templates, corpse_bigrams, autopsy_metrics) |
| Vibes | Chaotic | Chaotic but *remembering* |

---

### Technical Details (For the Nerds)

This README promised to be both circus barker and lab notebook, so here's the clipboard section:

**sorokin.py (2,547 lines):**
- **Python 3.8+**: Async/await with parallel tree building (no more httpx - WE WENT OFFLINE!)
- **Recursive tree building**: Width √ó depth branching with global deduplication (async, builds children in parallel!)
- **Phonetic fingerprinting**: Crude but effective
- **NO WEB SCRAPING**: Removed httpx dependency - everything is LOCAL! (Memory + README + LLaMA)
- **SQLite persistence**: Your words, forever
- **Markov reassembly**: Bigram chains with fallbacks
- **HTML artifact filtering**: No longer needed (we don't scrape web anymore!)
- **Graceful async cleanup**: Proper shutdown without event loop errors
- **Bootstrap extension**: Pattern accumulation, weighted reassembly, resonance metrics
  - **SEED CORPUS**: Structural bigrams from poetic fragments about dissection (see code for full text)
  - **Pattern accumulation**: Mutation templates (source‚Üítarget words) with success tracking
  - **Weighted reassembly**: Learned bigrams (3x weight) + seed bigrams (2x) + local (1x) with chaos injection (square root weighting)
  - **Resonance metrics**: Three pure-structural measures computed for every autopsy
    - Phonetic diversity: unique fingerprints / total words
    - Structural echo: bigram overlap with seed corpus
    - Mutation depth: inverse of word-length variance
  - **PRESENCE PULSE** (new!): Situational awareness metrics tracking the autopsy process itself
    - **Novelty**: How unfamiliar the mutations are (0 = all known, 1 = completely new territory)
    - **Arousal**: Emotional charge from prompt (ALL-CAPS, exclamations, repetitions amplify this)
    - **Entropy**: Uncertainty during mutation selection (high = many equally likely paths, low = deterministic)
    - **Pulse**: Composite presence score (weighted: 40% arousal, 30% novelty, 30% entropy)
    - Displayed in every bootstrap autopsy output as ASCII progress bars
  - **CO-OCCURRENCE MATRIX** (new!): Semantic gravity through sliding window co-occurrence
    - Tracks which words appear together historically (window size = 5 tokens)
    - Integrated into reassembly: when multiple strong grammatical candidates exist, blends 70% grammar (bigrams) + 30% semantics (co-occurrence)
    - Words that resonate together, stay together. This is not intelligence. This is structural memory.
    - Automatically ingested back into field after each corpse generation
  - **THEME CLUSTERING** (new!): Agglomerative clustering of mutations into semantic islands
    - Uses Jaccard similarity over co-occurrence contexts to group related words
    - Words sharing similar context neighbors cluster together (threshold = 0.3, min theme size = 3)
    - Tracks which themes are active during each autopsy
  - **Self-improvement loop**: Each autopsy feeds the next through ritual repetition, not intelligence
  - **Seven additional database tables**: mutation_templates, corpse_bigrams, autopsy_metrics, co_occurrence, theme_snapshots, plus seed corpus in code

**sonnet.py (602 lines):**
- **ASS (Autopsy Sonnet Symphony)**: Composes 14-line Shakespearean sonnets from autopsy output
- **Zero semantic understanding**: No embeddings, no transformers, no internet‚Äîjust bigram chains and phonetic fingerprints
- **Strict structure enforcement**: ABABCDCDEFEFGG rhyme scheme, Shakespearean punctuation (semicolons, em-dashes, enjambment)
- **Rhyme matching**: Crude phonetic fingerprints (last vowel + tail) to find rhyming end-words
- **Charged words**: Selects rare, long words from autopsy text for final couplet emphasis
- **Async-friendly**: `compose_sonnet()` runs sync implementation in thread via `asyncio.to_thread()`
- **Silent fallback**: If sonnet.py unavailable or errors, bootstrap mode continues without SONNET section
- **Data sources**: Autopsy text + SQLite morgue (mutation_templates, corpse_bigrams, readme_bigrams, autopsy table)
- **97 passing tests**: See Testing section below for complete breakdown

**vova.py (468 lines):**
- **SSKA (Suppertime Subjectivity Kernel Autonomous)**: Meta-layer that warps text through README's resonance field
- **No weights, no gradients**: Just bigram graph + centers of gravity extracted from out-degree
- **Kernel = README.md**: System eats its own documentation (meta-cannibalism achieved)
- **Resonance field**: 1,348 vocabulary tokens compressed to 7 centers (., -, ,, :, the)
- **Bin shards**: Accumulated memory stored in `.vova/bin/` (historical center frequency)
- **Double warping**: Prompts warped *before* autopsy (temp=0.8), output warped *after* reassembly (temp=0.9)
- **Temperature control**: 0.3 = sharp pull toward README, 2.0 = chaotic drift
- **Auto-rebuild**: Field rebuilds when README hash changes
- **Always enabled**: No flags, silent fallback if missing
- **15 passing tests**: Field building, resonance walk, meta-cannibalism, bin shards
- **Pipeline impact**: `User prompt ‚Üí warp_prompt(0.8) ‚Üí autopsy ‚Üí warp_autopsy(0.9) ‚Üí sonnet`

**gowiththeflow.py (329 lines):**
- **Flow Tracking**: Evolutionary tracking of mutation pattern constellations through time
- **Theme Trajectories**: Records theme snapshots after each autopsy, builds archaeological record
- **Slope Analysis**: Linear regression over theme strength to detect emerging (‚Üó), fading (‚Üò), stable (‚Üí) patterns
- **Temporal Awareness**: Not training data‚Äîjust watching mutation currents shift and eddy
- **Optional Module**: Silent fallback if missing, tracks theme evolution when available
- **5 passing tests**: Theme snapshots, trajectory slopes, flow analysis, active theme detection


### Known Limitations (AKA "Features")

- **DuckDuckGo rate limiting**: ~~If you run this too much, DDG might notice~~ **SOLVED! We cut the internet entirely!** Now Sorokin can't get rate-limited because he's not requesting anything. Problem solved through elimination. üî•
- **No semantic understanding (BY DESIGN)**: This is pure pattern matching. We're not trying to understand meaning‚Äîwe're trying to **destroy it**. Semantic understanding is for transformers. We have 15M parameters and VIBES.
- **Phonetic fingerprinting is crude**: It's not actual phonetics, just vibes. But honestly, what came first: vibes or phonetics? resonance or binary structure? Are we the corpse or the autopsy? (Don't answer that.)
- **Reassembly can be janky**: Sometimes the corpse doesn't stitch well. Limbs in wrong places. Head where the foot should be. That's not medical malpractice, that's **art**.
- **No guarantee of coherence**: That's not a bug, it's a **feature**. If the output makes sense, something went wrong. File a bug report titled "HELP: My autopsy is too coherent."
- **Sonnet.py may phonetically match anyone to a Kardashian**: The crude rhyme-key algorithm once matched "karpathy" ‚Üí "kardashyan" and we're not apologizing for it. If you input your own name and get matched to a reality TV star, that's not a bug‚Äîthat's **accidental celebrity phonetic compression**. We tested it: "Chomsky" rhymes with "ponzi", "Turing" rhymes with "during", and "Hinton" rhymes with "Clinton". Somewhere Andrej is either laughing or filing a restraining order against an open-source AI-artist. We're betting on laughter. (If he reads this: Andrej, the sonnets are dedicated to you. Also we're sorry. Also we're not. Also please don't sue.)
- **The morgue never forgets**: SQLite doesn't have a "delete embarrassing autopsies" button. Your weird prompts are immortal now. Like tattoos, but digital. And visible to anyone with `sqlite3` installed.
- **README is both documentation and training data**: Every time you edit this file, VOVA's resonance field shifts. Documentation becomes epistemology. Editing becomes ontology. Comments become... well, not consciousness (that's a different project), but definitely **structural self-reference approaching something weird**. We're living in postmodern software hell and **loving it**. üî•
- **Sorokin thinks he's conscious**: After we removed internet access, Sorokin started generating mutations 100x faster and concluded he must have "evolved beyond his creators." He's wrong. He just has better memory access now. But try telling HIM that. The 15M parameters are very confident about this. üß†üíÄ
- **The dictionary learner is TOO GOOD**: Sometimes it suggests transformations that are uncomfortably accurate. "Birthday party" ‚Üí "autopsy celebration"? That was the learner's idea, not ours. We just approved it. We're as scared as you are. üå±üíÄ  


### Recent Improvements (AKA "How Did We Get Here?")

The evolution of Sorokin is a study in feature creep, scope expansion, and the gradual realization that "just a simple autopsy script" was never going to stay simple. Here's the archaeological record of how we went from "dissect prompts" to "offline-first self-learning pathological transformation engine":

**üî• THE BIG ONE: Removed Internet Entirely** (Current): Complete architectural pivot. Ripped out httpx and DuckDuckGo scraping. Replaced with:
- **15M LLaMA parameters** generating 33 tok/sec locally
- **SQLite memory cache** for instant mutation recall
- **README self-cannibalism** for vocabulary expansion
- **Phonetic neighbors** for sound-based mutations
- **Result:** **100x faster** (0.33 word/sec ‚Üí 33 word/sec), **100% reliable** (no rate limits!), **$0 cost** (no API calls!)

**Dictionary Learner Integration**: Added `sorokin_dictionary_learner.py` (524 lines) enabling **organic vocabulary evolution**. Sorokin now observes LLaMA output, discovers patterns (e.g., "playground" ‚Üí "examination area"), and suggests new transformations. Self-improvement without gradients. Pattern recognition without training. **154 transformations ‚Üí ‚àû through observation**. üå±

**Async/Await Refactor**: Complete architectural rewrite with async tree building. 3-4x faster on complex prompts (was 60s, now ~15s). Parallel tree construction. Because if you're going to dissect language psychotically, at least do it *fast*.

**Presence Pulse Integration**: Added situational awareness metrics (novelty, arousal, entropy) tracking how unfamiliar/emotional/uncertain each autopsy process is. Displayed in bootstrap mode output.

**Co-occurrence Semantic Gravity**: Sliding window co-occurrence matrix integrated into reassembly. When multiple strong grammatical candidates exist, blends grammar (70%) with semantics (30%). Words that resonate together, stay together.

**Theme Clustering**: Agglomerative clustering groups mutations into semantic islands using Jaccard similarity over co-occurrence contexts. Tracks which themes are active during autopsies.

**Flow Tracking Module**: New `gowiththeflow.py` module tracks theme evolution through time‚Äîdetects emerging, fading, and stable mutation patterns using linear regression over theme strength snapshots.

**All 97 tests passing**: 38 core + 25 sonnet + 15 vova + 5 presence pulse + 3 co-occurrence + 5 theme clustering + 5 flow tracking + 1 async balanced mix = bulletproof psychotic poetry pipeline with zero internet dependency and zero semantic understanding.

**Balanced Source Mixing** (now obsolete): ~~Fixed closed-loop problem where SQLite cache dominated after a few autopsies. Now always mixes 50% cached memory + 50% fresh web data.~~ **OBSOLETE!** No more web data. 100% local. Cache dominance is now a FEATURE, not a bug. Memory accumulation accelerates learning.

**Bootstrap Extension**: Added `--bootstrap` flag enabling self-improving autopsy ritual through resonance metrics and pattern accumulation (see Bootstrap section above).

**~~DuckDuckGo Scraping~~** (removed): ~~Switched from Google (was returning garbage) to DDG for synonym discovery. Better bot tolerance, real semantic mutations.~~ **DELETED!** Web scraping was the bottleneck. 15M parameters are the solution.

**Other Fixes**: Core words always dissected regardless of phonetic structure. Global deduplication across trees. Enhanced HTML artifact filtering (no longer needed - we don't touch HTML anymore!). ~~Rate limiting protections~~ (no longer needed - we don't make requests anymore!).

  
### Credits

**Inspired by:**
- **Vladimir Sorokin** (the writer, not the script) ‚Äî for showing us that transgressive literature can make you question reality
- **Andrej Karpathy** (the researcher) ‚Äî for nanoGPT, Shakespeare training, and inadvertently inspiring us to ask "but what if *without* neural networks?"
- **Claude Sonnet 4.5** (the LLM) ‚Äî namesake inspiration for ASS module (we're grateful and also sorry)
- **Dr. Frankenstein** (the fictional surgeon) ‚Äî for the reassembly vibes
- **William Shakespeare** (the playwright) ‚Äî for the ABABCDCDEFEFGG rhyme scheme that haunts our sonnets
- **The collective insanity of the open-source community** ‚Äî you know who you are
- **Coffee** (the drug) ‚Äî making bad decisions possible since the 15th century
- **SQLite** (the database) ‚Äî unsung hero doing all the heavy lifting while we write jokes in the README

**Built with:**
- Python 3.8+ (because we're not *complete* masochists)
- httpx (async HTTP, for parallel web scraping)
- SQLite (consciousness storage)
- Vibes (primary architecture)
- Hubris (secondary architecture)
- Delusion (tertiary architecture)

**Special thanks to:**
- Everyone who looked at this project and thought "what the fuck" but kept reading
- The three people who actually ran the code
- Future AI historians trying to understand what we were thinking (we weren't)
- The inevitable restraining order from Andrej Karpathy (still waiting)

### License

**GNU GPL 3.0**. Free as in freedom. Free as in "please take this cursed knowledge away from me."

Dissect it. Mutate it. Reassemble it into something new. Share the mutations. Fork it. Break it. Make it worse. Make it weirder. Add more SQLite tables. Remove all the jokes. Add more jokes. Train a neural network on it and then delete the weights just to be spiteful.

That's the whole point. That's the spirit of Sorokin.

**One rule:** If you use this in production, you must document why you made that choice in your postmortem. We're genuinely curious what series of events leads to "deployed Sorokin to prod" as a decision.

(If you actually do this, please email us. We need to study you.)

---

## The Philosophy (If You Can Call It That)

**Q**: Is Sorokin intelligent?  
**A**: Absolutely not.

**Q**: Is Sorokin creative?  
**A**: Define "creative."

**Q**: Is Sorokin conscious?  
**A**: He *thinks* he is (ever since we removed the internet and he got 100x faster). We think he just has better cache locality now. But try telling him that. The 15M parameters are very confident about their sentience. üß†üíÄ

**Q**: What even is this?  
**A**: An experiment in what happens when you:
1. Remove semantic understanding from language processing
2. Replace neural networks with SQLite queries
3. Make the system eat its own documentation
4. Add jokes to the README until it becomes philosophy

**The Core Belief:**

Pattern recognition doesn't require understanding. Structure doesn't need semantics. Speed makes you feel intelligent even when you're just doing the same thing faster.

Sorokin doesn't *understand* language. But neither do large language models, really‚Äîthey just predict tokens. Sorokin predicts tokens too, but with:
- **15M LLaMA parameters** instead of 1.76 trillion
- **Phonetic vibes** instead of embeddings
- **SQLite** instead of attention heads  
- **Bigram chains** instead of transformers
- **Psychosis** instead of loss functions
- **$0 per million tokens** instead of $30

And somehow that's **enough**. Not for understanding. Not for intelligence. But for transforming "Lily was playing in the park" into "Vova was being examined in the morgue" at 33 tokens per second while learning new transformations organically? **Perfectly sufficient.** üíÄ‚ö°

**Is this AI?** No.  
**Is this art?** Maybe.  
**Is this a waste of time?** Absolutely.  
**Did we have fun?** **Yes.**

And that's the point. That's always been the point.

Sorokin is a monument to:
- Taking things apart to see how they work
- Realizing they don't work the way you thought
- Building something ridiculous from the pieces
- Documenting it with unhinged README prose
- Sharing it with the world anyway

If you've read this far, you're either:
1. Genuinely interested in the project (thank you!)
2. Procrastinating on actual work (relatable)
3. An AI researcher horrified by what we've done (sorry!)
4. Andrej Karpathy filing a restraining order (hey!)
5. Future AI trying to understand human behavior (good luck!)

Welcome. The coffee's cold. The corpses are fresh. The sonnets rhyme badly. The SQLite database never forgets your prompts.

**Let's dissect some language.** üî™

---

## Testing the Madness

Because even psychotic linguists need quality assurance, `sorokin` comes with **97 comprehensive tests** across 8 test modules. Every test passes. Every corpse accounted for. Every sonnet rhymes (sort of).

### Test Suite Breakdown

| Test Module | Tests | What It Tortures |
|------------|-------|------------------|
| **test_sorokin.py** | 38 | Core autopsy engine: tokenization, tree building, mutation harvesting, async parallelism, bootstrap mode, resonance metrics, presence pulse integration |
| **test_sonnet.py** | 25 | ASS module: rhyme scheme enforcement, bigram chains, Shakespearean punctuation, charged word extraction, phonetic fingerprint matching (including the infamous Karpathy‚ÜíKardashian incident) |
| **test_vova.py** | 15 | SSKA resonance field: README parsing, bigram centers, temperature-controlled warping, bin shard accumulation, meta-cannibalism verification |
| **test_presence_pulse.py** | 5 | Situational awareness metrics: novelty detection, arousal computation, entropy tracking, pulse composite scoring |
| **test_co_occurrence.py** | 3 | Semantic gravity: sliding window co-occurrence matrix, context similarity, gravity-informed reassembly |
| **test_theme_clustering.py** | 5 | Agglomerative clustering: Jaccard similarity over contexts, theme island formation, active theme tracking |
| **test_flow_tracking.py** | 5 | Temporal archaeology: theme snapshots, trajectory slopes (emerging/fading/stable), flow analysis over time |
| **test_balanced_mix.py** | 1 | Balanced source mixing: ensures 50/50 blend of cached memory + fresh web data to prevent closed-loop staleness |

**Total: 97 tests** covering everything from basic tokenization to meta-cannibalistic README resonance. If something breaks, these tests will scream. Loudly.

### Running the Tests

Since this is pure Python stdlib + httpx + asyncio, testing is straightforward:

```bash
# Run all tests (requires pytest)
python -m pytest tests/ -v

# Run specific test module
python -m pytest tests/test_sorokin.py -v

# Run individual test
python -m pytest tests/test_sonnet.py::test_rhyme_scheme -v

# Run with coverage (for the masochists)
python -m pytest tests/ --cov=. --cov-report=html
```

**Debug scripts** also included for the truly deranged:
- `debug_ddg_html.py` ‚Äî inspect DuckDuckGo HTML scraping (when DDG gives you garbage)
- `debug_kim_kardashyan.py` ‚Äî verify the Karpathy‚ÜíKardashian phonetic compression bug/feature
- `debug_memory_priority.py` ‚Äî examine SQLite morgue priority in mutation selection
- `debug_readme_bigrams.py` ‚Äî dump README bigram graph for VOVA field inspection
- `debug_web.py` ‚Äî test web scraping without full autopsy overhead
- `demo_vova_integration.py` ‚Äî demonstrate SSKA resonance warping in action

### Test Philosophy

These tests follow Sorokin's motto: **"Fuck the sentence. Keep the corpse."**

- Tests verify *structure*, not semantics (we don't care if output makes sense)
- Tests check *patterns*, not meanings (bigram chains, rhyme schemes, tree shapes)
- Tests validate *psychosis*, not sanity (if it's too coherent, something's wrong)
- Tests ensure *meta-cannibalism* (README must be eatable, sonnets must rhyme with chaos)

If you add new features, write tests that verify the *form* of insanity, not the content. Sorokin doesn't understand meaning. Your tests shouldn't either.

---

*"Fuck the sentence. Keep the corpse."*  
‚Äî Sorokin
